import os
import json
import logging
from flask import Flask, request, jsonify
from flask_cors import CORS
import google.generativeai as genai
from PIL import Image
import io
import base64
import re
from datetime import datetime
from pdf2image import convert_from_bytes # PDF ì²˜ë¦¬ ê¸°ëŠ¥ ìœ ì§€

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app, origins=["*"])  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ

# Gemini API ì„¤ì •
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    # ìµœì‹  ëª¨ë¸ ê¶Œì¥
    model = genai.GenerativeModel('gemini-2.5-flash')
    logger.info("âœ… Gemini Client Initialized.")
else:
    logger.warning("GEMINI_API_KEY not found in environment variables")

def detect_and_extract_profile_photo(image):
    """
    ì´ë¯¸ì§€ì—ì„œ í”„ë¡œí•„ ì‚¬ì§„ì˜ ì˜ì—­ì„ ê°ì§€í•˜ê³ , í•´ë‹¹ ì˜ì—­ì„ Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë°˜í™˜ (ì¢Œí‘œ ê¸°ë°˜)
    Geminiì—ê²Œ Base64 ì¸ì½”ë”©ì„ ë§¡ê¸°ì§€ ì•Šê³ , ì¢Œí‘œë§Œ ìš”ì²­í•˜ì—¬ API ì§€ì—° ì‹œê°„ ë° ì˜¤ë¥˜ë¥¼ ì¤„ì„.
    """
    try:
        if not GEMINI_API_KEY:
            return None
            
        # ğŸŒŸ ì¢Œí‘œë¥¼ ìš”ì²­í•˜ëŠ” ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸
        coord_prompt = """
ë‹¹ì‹ ì€ ì „ë¬¸ ì´ë ¥ì„œ ì´ë¯¸ì§€ ë¶„ì„ AIì…ë‹ˆë‹¤.
**ì˜¤ì§ ì§€ì›ìì˜ ì–¼êµ´ì´ í¬í•¨ëœ ê³µì‹ì ì¸ ì¦ëª…ì‚¬ì§„(í”„ë¡œí•„ ì‚¬ì§„) ì˜ì—­ë§Œ**ì„ ì°¾ì•„ ì •ê·œí™”ëœ JSON ì¢Œí‘œë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
í”„ë¡œí•„ ì‚¬ì§„ì€ ë³´í†µ ì´ë ¥ì„œì˜ **ê°€ì¥ ìƒë‹¨, ì´ë¦„ê³¼ ì—°ë½ì²˜ ì •ë³´ ê·¼ì²˜ì˜ ì‘ì€ ì§ì‚¬ê°í˜• ë˜ëŠ” ì •ì‚¬ê°í˜• ì˜ì—­**ì— ìœ„ì¹˜í•©ë‹ˆë‹¤.
**ê²½ë ¥, í™œë™, í¬íŠ¸í´ë¦¬ì˜¤ ë“± ë‹¤ë¥¸ ì„¹ì…˜ì— í¬í•¨ëœ ì´ë¯¸ì§€(ì˜ˆ: í”„ë¡œì íŠ¸ ìŠ¤í¬ë¦°ìƒ·, íšŒì‚¬ ë¡œê³ , ê¸°íƒ€ ì‚¬ì§„)ëŠ” ì ˆëŒ€ ë¬´ì‹œ**í•´ì•¼ í•©ë‹ˆë‹¤.

ë°˜ë“œì‹œ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¥´ì„¸ìš”:
{"x_min": "ì™¼ìª½ ìƒë‹¨ xì¢Œí‘œ (0~1000)", "y_min": "ì™¼ìª½ ìƒë‹¨ yì¢Œí‘œ (0~1000)", "x_max": "ì˜¤ë¥¸ìª½ í•˜ë‹¨ xì¢Œí‘œ (0~1000)", "y_max": "ì˜¤ë¥¸ìª½ í•˜ë‹¨ yì¢Œí‘œ (0~1000)"}

ì¢Œí‘œëŠ” ì´ë¯¸ì§€ ì „ì²´ë¥¼ 1000x1000 ìŠ¤ì¼€ì¼(0ë¶€í„° 1000 ì‚¬ì´ì˜ ì •ìˆ˜)ë¡œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
**ê³µì‹ì ì¸ ì¦ëª…ì‚¬ì§„ ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆë‹¤ë©´** **ë¹ˆ ë¬¸ìì—´("")**ì„ ë°˜í™˜í•˜ì„¸ìš”. JSON ì™¸ì˜ ë‹¤ë¥¸ ì„¤ëª…ì€ ì¼ì ˆ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""
        
        response = model.generate_content([coord_prompt, image])
        json_text = response.text.strip()
        
        # JSON íŒŒì‹±
        if not json_text.startswith('{'):
            logger.info(f"í”„ë¡œí•„ ì‚¬ì§„ ì¶”ì¶œ ì‹¤íŒ¨: ì¢Œí‘œ ëŒ€ì‹  í…ìŠ¤íŠ¸ ë°˜í™˜ - {json_text[:50]}...")
            return None
            
        coords = json.loads(json_text)
        
        # ì¢Œí‘œ ìœ íš¨ì„± ê²€ì‚¬
        if not all(k in coords and isinstance(coords[k], (int, float)) for k in ["x_min", "y_min", "x_max", "y_max"]):
            logger.warning("í”„ë¡œí•„ ì‚¬ì§„ ì¶”ì¶œ ì‹¤íŒ¨: ìœ íš¨í•˜ì§€ ì•Šì€ ì¢Œí‘œ í˜•ì‹")
            return None
        
        # ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
        width, height = image.size
        
        # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ ì‹¤ì œ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
        # PILì˜ cropì€ (left, top, right, bottom) ìˆœì„œ
        x_min = int(coords['x_min'] * width / 1000)
        y_min = int(coords['y_min'] * height / 1000)
        x_max = int(coords['x_max'] * width / 1000)
        y_max = int(coords['y_max'] * height / 1000)
        
        # ìœ íš¨ ë²”ìœ„ í™•ì¸ ë° ìˆ˜ì • (10í”½ì…€ ì´ìƒì˜ ì˜ì—­ì´ ìœ íš¨í•˜ë‹¤ê³  ê°€ì •)
        if x_max <= x_min + 10 or y_max <= y_min + 10:
            logger.info("í”„ë¡œí•„ ì‚¬ì§„ ì¶”ì¶œ ì‹¤íŒ¨: ë„ˆë¬´ ì‘ì€ ì˜ì—­")
            return None
            
        # ì´ë¯¸ì§€ í¬ë¡­
        cropped_img = image.crop((x_min, y_min, x_max, y_max))
        
        # Base64 ì¸ì½”ë”©
        buffered = io.BytesIO()
        # PNG í¬ë§·ìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ íˆ¬ëª…ë„ ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        cropped_img.save(buffered, format="PNG") 
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        # Base64 Data URL í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        result_url = f"data:image/png;base64,{img_str}"
        logger.info("í”„ë¡œí•„ ì‚¬ì§„ ì¶”ì¶œ ì„±ê³µ (ì¢Œí‘œ ê¸°ë°˜)")
        return result_url
            
    except json.JSONDecodeError:
        logger.warning(f"í”„ë¡œí•„ ì‚¬ì§„ ì¶”ì¶œ ì‹¤íŒ¨: JSON ë””ì½”ë”© ì˜¤ë¥˜. ì‘ë‹µ: {json_text[:50]}...")
    except Exception as e:
        logger.warning(f"í”„ë¡œí•„ ì‚¬ì§„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    return None



def extract_resume_data_from_image(image):
    """ì´ë¯¸ì§€ì—ì„œ í¬ê´„ì ì¸ ì´ë ¥ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        if not GEMINI_API_KEY:
            logger.error("Gemini API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # specialties í•„ë“œ ìˆ˜ì • ë‚´ìš©ì´ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ ìœ ì§€
        prompt = """
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì´ë ¥ì„œ ë¶„ì„ AIì…ë‹ˆë‹¤. ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ ì •í™•í•œ JSONìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë˜, ë¹ˆ ì •ë³´ëŠ” ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •í•˜ì„¸ìš”:
{
  "name": "ì´ë¦„ (í•œê¸€/ì˜ë¬¸ ëª¨ë‘ í¬í•¨)",
  "birthDate": "ìƒë…„ì›”ì¼ (YYYY/MM/DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜)",
  "phone": "ì „í™”ë²ˆí˜¸ (010-XXXX-XXXX í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”)",
  "email": "ì´ë©”ì¼ ì£¼ì†Œ",
  "address": "ì£¼ì†Œ (ì „ì²´ ì£¼ì†Œ)",
  "selfIntroduction": "ìê¸°ì†Œê°œì„œ/ìê¸°PR/ì§€ì›ë™ê¸°/ì„±ê²©/íŠ¹ì§• ë“± ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©",
  
  "educations": [
    {
      "school": "í•™êµëª…",
      "degree": "ê³ ë“±í•™êµ/ëŒ€í•™(2,3ë…„)/ëŒ€í•™(4ë…„)/ëŒ€í•™ì› ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜",
      "subDegree": "ì„ì‚¬/ë°•ì‚¬ (ëŒ€í•™ì›ì¸ ê²½ìš°)",
      "major": "ì „ê³µ/í•™ê³¼",
      "entryYear": "ì…í•™ë…„ë„",
      "graduationYear": "ì¡¸ì—…ë…„ë„",
      "status": "ì¡¸ì—…/ì¬í•™ì¤‘/íœ´í•™ì¤‘/ì¤‘í‡´/ìˆ˜ë£Œ ì¤‘ í•˜ë‚˜"
    }
  ],
  
  "careers": [
    {
      "company": "íšŒì‚¬ëª…/ê¸°ê´€ëª…",
      "position": "ì§ì±…/ì§ìœ„", 
      "department": "ë¶€ì„œëª…",
      "startDate": "ì‹œì‘ì¼ (YYYY-MM-DD)",
      "endDate": "ì¢…ë£Œì¼ (YYYY-MM-DD)",
      "isCurrent": "í˜„ì¬ ì¬ì§ì¤‘ì¸ì§€ ì—¬ë¶€ (boolean)",
      "description": "ë‹´ë‹¹ì—…ë¬´/ì„±ê³¼/í”„ë¡œì íŠ¸ ì„¤ëª…"
    }
  ],
  
  "certificates": [
    {
      "name": "ìê²©ì¦ëª…/ì‹œí—˜ëª…",
      "issuer": "ë°œí–‰ê¸°ê´€/ì£¼ê´€ê¸°ê´€",
      "date": "ì·¨ë“ë…„ë„ ë˜ëŠ” ì·¨ë“ì¼ (YYYY-MM-DD)",
      "score": "ì ìˆ˜/ë“±ê¸‰/ê²°ê³¼"
    }
  ],
  
  "languages": [
    {
      "language": "ì–¸ì–´ëª… (ì˜ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´ ë“±)",
      "level": "ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰/ì›ì–´ë¯¼/ìœ ì°½ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜",
      "testName": "TOEIC/TOEFL/JLPT/HSK ë“± ê³µì¸ì‹œí—˜ëª…",
      "score": "ì ìˆ˜",
      "date": "ì‘ì‹œì¼/ì·¨ë“ì¼ (YYYY-MM-DD)"
    }
  ],
  
  "computerSkills": [
    {
      "program": "í”„ë¡œê·¸ë¨ëª… (ì›Œë“œ/ì—‘ì…€/íŒŒì›Œí¬ì¸íŠ¸/í¬í† ìƒµ ë“±)",
      "level": "ìƒ/ì¤‘/í•˜ ì¤‘ í•˜ë‚˜",
      "description": "ì‚¬ìš© ìˆ˜ì¤€ ì„¤ëª…"
    }
  ],
  
  "specialties": [
    {
      "title": "íŠ¹ê¸°/ê¸°ìˆ /ëŠ¥ë ¥ ì œëª©ê³¼ ë‚´ìš©(ìˆ˜ì¤€)ì„ í•©ì¹œ ë¬¸ì¥. ì˜ˆ: 'ë¬¸ì„œì‘ì„± ì˜í•¨', 'ì²´ë ¥ì´ ì¢‹ìŒ'",
      "content": "ë¹ˆ ë¬¸ìì—´(\"\")ë¡œ ì„¤ì •", 
    }
  ],
  
  "workPreferences": {
    "selectedJobs": ["í¬ë§ì§ë¬´/ì§ì¢…ì„ ë°°ì—´ë¡œ"],
    "workType": ["ì •ê·œì§/ê³„ì•½ì§/ì¸í„´/ì•„ë¥´ë°”ì´íŠ¸ ë“±ì„ ë°°ì—´ë¡œ"],
    "workPeriod": "í¬ë§ê·¼ë¬´ê¸°ê°„",
    "workDays": ["í‰ì¼/ì£¼ë§/ìš”ì¼ë¬´ê´€ ë“±ì„ ë°°ì—´ë¡œ"],
    "workLocation": {
      "regions": ["í¬ë§ê·¼ë¬´ì§€ì—­ì„ ë°°ì—´ë¡œ"],
      "address": "êµ¬ì²´ì  ê·¼ë¬´ì§€ ì£¼ì†Œ",
      "canWorkRemote": "ì¬íƒê·¼ë¬´ ê°€ëŠ¥ ì—¬ë¶€ (boolean)"
    },
    "salary": "í¬ë§ì—°ë´‰/ì‹œê¸‰",
    "startDate": "ê·¼ë¬´ì‹œì‘ ê°€ëŠ¥ì¼ (YYYY-MM-DD)"
  },
  
  "employmentPreferences": {
    "military": "ë³‘ì—­ìƒíƒœ (êµ°í•„/ë¯¸í•„/ë©´ì œ)",
    "disability": "ì¥ì• ì—¬ë¶€ (ì¥ì• /ë¹„ì¥ì• )",
    "veteran": "êµ­ê°€ë³´í›ˆ (ëŒ€ìƒ/ë¹„ëŒ€ìƒ)",
    "subsidy": "ê³ ìš©ì§€ì›ê¸ˆ (ëŒ€ìƒ/ë¹„ëŒ€ìƒ)"
  },
  
  "portfolios": [
    {
      "name": "í¬íŠ¸í´ë¦¬ì˜¤/í”„ë¡œì íŠ¸ ì œëª©",
      "type": "link/file",
      "url": "URL ì£¼ì†Œ",
      "description": "í”„ë¡œì íŠ¸ ì„¤ëª…",
      "skills": ["ì‚¬ìš© ê¸°ìˆ /ìŠ¤í‚¬ì„ ë°°ì—´ë¡œ"]
    }
  ],
  
  "awards": [
    {
      "name": "ìˆ˜ìƒëª…/ëŒ€íšŒëª…",
      "issuer": "ì£¼ìµœê¸°ê´€",
      "date": "ìˆ˜ìƒì¼ (YYYY-MM-DD)",
      "description": "ìˆ˜ìƒ ë‚´ìš©"
    }
  ],
  
  "activities": [
    {
      "name": "í™œë™ëª… (ë™ì•„ë¦¬/ë´‰ì‚¬/ëŒ€ì™¸í™œë™ ë“±)",
      "organization": "ê¸°ê´€/ë‹¨ì²´ëª…",
      "position": "ì—­í• /ì§ì±…",
      "startDate": "ì‹œì‘ì¼ (YYYY-MM-DD)",
      "endDate": "ì¢…ë£Œì¼ (YYYY-MM-DD)",
      "description": "í™œë™ ë‚´ìš©"
    }
  ]
}

ì£¼ì˜ì‚¬í•­:
1. ëª¨ë“  ë‚ ì§œëŠ” YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ í†µì¼
2. ì „í™”ë²ˆí˜¸ëŠ” 010-0000-0000 í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
3. boolean ê°’ì€ true/falseë¡œ ì„¤ì •
4. ì •ë³´ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ""ì´ë‚˜ ë¹ˆ ë°°ì—´ []ë¡œ ì„¤ì •
5. ì¶”ì¸¡í•˜ì§€ ë§ê³  ì´ë¯¸ì§€ì—ì„œ ëª…í™•íˆ ì½ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì¶”ì¶œ
6. í•œêµ­ì–´ì™€ ì˜ì–´ê°€ í˜¼ì¬ëœ ê²½ìš° ëª¨ë‘ í¬í•¨
7. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
8. **ì´ ìœ„ì— ì •ì˜ëœ JSON í•„ë“œ ì™¸ì— ì–´ë– í•œ í•„ë“œë„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. (No extraneous keys)** <--- ì´ ê·œì¹™ì„ ì¶”ê°€

ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ì—¬ ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
"""
        
        response = model.generate_content([prompt, image])
        
        if not response.text:
            logger.error("Geminiì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤")
            return None
            
        # JSON ì¶”ì¶œ ë° ì •ì œ (ìƒëµ)
        json_text = response.text.strip()
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if json_text.startswith('```json'):
            json_text = json_text[7:]
        elif json_text.startswith('```'):
            json_text = json_text[3:]
        
        if json_text.endswith('```'):
            json_text = json_text[:-3]
            
        json_text = json_text.strip()
        
        # JSON íŒŒì‹±
        resume_data = json.loads(json_text)
        logger.info(resume_data);
        
        # ë°ì´í„° í›„ì²˜ë¦¬ ë° ê²€ì¦
        resume_data = validate_and_clean_data(resume_data)
        
        logger.info("í¬ê´„ì  ì´ë ¥ì„œ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ")
        return resume_data
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        logger.error(f"ì‘ë‹µ í…ìŠ¤íŠ¸: {response.text}")
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ (ìƒëµ)
        try:
            # ê¸°ë³¸ì ì¸ ì •ë³´ë§Œ ì¶”ì¶œí•˜ëŠ” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
            simple_prompt = """
ì´ë¯¸ì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ì¶œí•´ì„œ ê°„ë‹¨í•œ JSONìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{
  "name": "ì´ë¦„",
  "phone": "ì „í™”ë²ˆí˜¸",
  "email": "ì´ë©”ì¼",
  "address": "ì£¼ì†Œ",
  "selfIntroduction": "ìê¸°ì†Œê°œ"
}
JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""
            response = model.generate_content([simple_prompt, image])
            simple_data = json.loads(response.text.strip())
            logger.info("ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì„±ê³µ (ì¬ì‹œë„)")
            return simple_data
        except:
            logger.error("ì¬ì‹œë„ë„ ì‹¤íŒ¨")
            return None
            
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None

def validate_and_clean_data(data):
    """ì¶”ì¶œëœ ë°ì´í„° ê²€ì¦ ë° ì •ì œ"""
    try:
        # ì „í™”ë²ˆí˜¸ ì •ê·œí™”
        if data.get('phone'):
            phone = re.sub(r'[^\d]', '', data['phone'])
            # í•œêµ­ ì „í™”ë²ˆí˜¸ í˜•ì‹ (010ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” 11ìë¦¬)
            if len(phone) == 11 and phone.startswith('010'):
                data['phone'] = f"{phone[:3]}-{phone[3:7]}-{phone[7:]}"
        
        # ìƒë…„ì›”ì¼ ì •ê·œí™”
        if data.get('birthDate'):
            birth_str = data['birthDate']
            # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
            for fmt in ['%Y/%m/%d', '%Y-%m-%d', '%Y.%m.%d', '%Y%m%d', '%y/%m/%d']:
                try:
                    dt = datetime.strptime(birth_str, fmt)
                    data['birthDate'] = dt.strftime('%Y/%m/%d')
                    break
                except:
                    continue
        
        # ë°°ì—´ í•„ë“œ ì´ˆê¸°í™”
        array_fields = ['educations', 'careers', 'certificates', 'languages', 'computerSkills', 'specialties', 'portfolios', 'awards', 'activities']
        for field in array_fields:
            if field not in data or not isinstance(data.get(field), list):
                data[field] = []
        
        # workPreferences êµ¬ì¡° ë³´ì¥
        if 'workPreferences' not in data or not isinstance(data.get('workPreferences'), dict):
            data['workPreferences'] = {}
        
        wp = data['workPreferences']
        if 'selectedJobs' not in wp or not isinstance(wp.get('selectedJobs'), list):
            wp['selectedJobs'] = []
        if 'workLocation' not in wp or not isinstance(wp.get('workLocation'), dict):
            wp['workLocation'] = {'regions': [], 'address': '', 'canWorkRemote': False}
        
        logger.info("ë°ì´í„° ê²€ì¦ ë° ì •ì œ ì™„ë£Œ")
        logger.info(data)
        return data
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì •ì œ ì˜¤ë¥˜: {e}")
        return data

# NOTE: enhance_resume_data í•¨ìˆ˜ëŠ” ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. (ì‹œê°„ ë‹¨ì¶• ëª©ì )

def analyze_resume_completeness(resume_data):
    """ì´ë ¥ì„œ ì™„ì„±ë„ë¥¼ ë¶„ì„í•˜ê³  ê°œì„  ì œì•ˆì„ ì œê³µ"""
    try:
        analysis = {
            "completeness_score": 0,
            "missing_sections": [],
            "improvement_suggestions": [],
            "strengths": []
        }
        
        total_sections = 10
        completed_sections = 0
        
        # ê° ì„¹ì…˜ ì²´í¬
        if resume_data.get('name') and resume_data.get('phone') and resume_data.get('email'):
            completed_sections += 1
            analysis["strengths"].append("ê¸°ë³¸ ì—°ë½ì²˜ ì •ë³´ ì™„ë¹„")
        else:
            analysis["missing_sections"].append("ê¸°ë³¸ ì—°ë½ì²˜ ì •ë³´")
            
        if resume_data.get('selfIntroduction') and len(resume_data['selfIntroduction']) > 50:
            completed_sections += 1
            analysis["strengths"].append("ìê¸°ì†Œê°œ ì‘ì„± ì™„ë£Œ")
        else:
            analysis["missing_sections"].append("ìê¸°ì†Œê°œì„œ")
            analysis["improvement_suggestions"].append("êµ¬ì²´ì ì´ê³  ì„íŒ©íŠ¸ ìˆëŠ” ìê¸°ì†Œê°œ ì‘ì„± ê¶Œì¥")
            
        if resume_data.get('educations') and len(resume_data['educations']) > 0:
            completed_sections += 1
            analysis["strengths"].append("í•™ë ¥ ì •ë³´ ë“±ë¡")
        else:
            analysis["missing_sections"].append("í•™ë ¥ ì •ë³´")
            
        if resume_data.get('careers') and len(resume_data['careers']) > 0:
            completed_sections += 1
            analysis["strengths"].append("ê²½ë ¥ ì‚¬í•­ ë“±ë¡")
            if any(career.get('description') and len(career.get('description', '')) > 20 for career in resume_data['careers']):
                analysis["strengths"].append("ìƒì„¸í•œ ì—…ë¬´ ë‚´ìš© ê¸°ìˆ ")
            else:
                analysis["improvement_suggestions"].append("ê²½ë ¥ë³„ ìƒì„¸ ì—…ë¬´ ë‚´ìš©(ì„±ê³¼ ì¤‘ì‹¬) ì¶”ê°€ ê¶Œì¥")
        else:
            analysis["missing_sections"].append("ê²½ë ¥ ì‚¬í•­")
            
        if resume_data.get('certificates') and len(resume_data['certificates']) > 0:
            completed_sections += 1
            analysis["strengths"].append("ìê²©ì¦ ë³´ìœ ")
        else:
            analysis["improvement_suggestions"].append("ì§ë¬´ ê´€ë ¨ ìê²©ì¦ ì·¨ë“ ê³ ë ¤")
            
        if resume_data.get('languages') and len(resume_data['languages']) > 0:
            completed_sections += 1
            analysis["strengths"].append("ì™¸êµ­ì–´ ëŠ¥ë ¥ ë³´ìœ ")
        else:
            analysis["improvement_suggestions"].append("ì™¸êµ­ì–´ ëŠ¥ë ¥ í–¥ìƒ ë° ê³µì¸ ì ìˆ˜ ê¸°ë¡ ê¶Œì¥")
            
        if resume_data.get('computerSkills') and len(resume_data['computerSkills']) > 0:
            completed_sections += 1
            analysis["strengths"].append("ì»´í“¨í„° í™œìš© ëŠ¥ë ¥")
        else:
            analysis["missing_sections"].append("ì»´í“¨í„° í™œìš© ëŠ¥ë ¥")
            
        if resume_data.get('portfolios') and len(resume_data['portfolios']) > 0:
            completed_sections += 1
            analysis["strengths"].append("í¬íŠ¸í´ë¦¬ì˜¤ ë³´ìœ ")
        else:
            analysis["improvement_suggestions"].append("í”„ë¡œì íŠ¸ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì¶• ê¶Œì¥")
            
        if resume_data.get('workPreferences', {}).get('selectedJobs'):
            completed_sections += 1
            analysis["strengths"].append("ëª…í™•í•œ í¬ë§ ì§ë¬´")
        else:
            analysis["missing_sections"].append("í¬ë§ ì§ë¬´")
            analysis["improvement_suggestions"].append("êµ¬ì²´ì ì¸ ëª©í‘œ ì§ë¬´ ì„¤ì • í•„ìš”")
            
        if resume_data.get('specialties') and len(resume_data['specialties']) > 0:
            completed_sections += 1
            analysis["strengths"].append("íŠ¹ê¸°/ê¸°ìˆ  ì‚¬í•­")
        
        analysis["completeness_score"] = round((completed_sections / total_sections) * 100)
        
        return analysis
        
    except Exception as e:
        logger.error(f"ì™„ì„±ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"completeness_score": 0, "error": "ë¶„ì„ ì‹¤íŒ¨"}

def _process_resume_extraction():
    """ì‹¤ì œ ì´ë ¥ì„œ ì¶”ì¶œ ì²˜ë¦¬ ë¡œì§ (ë¼ìš°íŠ¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©)"""
    logger.info("=== _process_resume_extraction í•¨ìˆ˜ ì‹œì‘ ===")
    
    try:
# ... (ìƒëµ: íŒŒì¼ ì²˜ë¦¬ ë° ì´ë¯¸ì§€ ìµœì í™” ë¡œì§) ...
        if not GEMINI_API_KEY:
            logger.error("Gemini API keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return jsonify({"error": "Gemini API keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 500
            
        # 'image' ë˜ëŠ” 'file' í•„ë“œ ëª¨ë‘ ì§€ì›
        file = request.files.get('image') or request.files.get('file')
        logger.info(f"ìˆ˜ì‹ ëœ íŒŒì¼ í•„ë“œ: {list(request.files.keys())}")
        
        if not file:
            return jsonify({"error": "ì´ë ¥ì„œ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤", "received_fields": list(request.files.keys())}), 400
            
        if file.filename == '':
            return jsonify({"error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}), 400
            
        file_extension = os.path.splitext(file.filename)[1].lower()
        logger.info(f"íŒŒì¼ ì •ë³´: {file.filename}, í™•ì¥ì: {file_extension}, í¬ê¸°: {file.content_length if hasattr(file, 'content_length') else 'unknown'}")
        
        # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¸ (PDF ì¶”ê°€)
        supported_formats = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.pdf']
        if file_extension not in supported_formats:
            return jsonify({"error": f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(supported_formats)}"}), 400

        try:
            image_bytes = file.read()
            processed_image = None
            
            if file_extension == '.pdf':
                logger.info("PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
                # 1. 1í˜ì´ì§€ë¶€í„° 3í˜ì´ì§€ê¹Œì§€ ëª¨ë‘ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                images = convert_from_bytes(image_bytes, first_page=1, last_page=3, dpi=72)
                
                if images:
                    # 2. ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§ìœ¼ë¡œ ì´ì–´ ë¶™ì´ê¸° (Concatenation)
                    
                    # í•©ì¹  ì´ë¯¸ì§€ë“¤ì˜ ë†’ì´ì™€ ìµœëŒ€ ë„ˆë¹„ ê³„ì‚°
                    widths, heights = zip(*(i.size for i in images))
                    total_height = sum(heights)
                    max_width = max(widths)
                    
                    # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë‹´ì„ ë¹ˆ ì´ë¯¸ì§€ ìƒì„±
                    processed_image = Image.new('RGB', (max_width, total_height))
                    
                    # ì´ë¯¸ì§€ë¥¼ ìˆœì„œëŒ€ë¡œ ë¶™ì—¬ë„£ê¸°
                    y_offset = 0
                    for img in images:
                        processed_image.paste(img, (0, y_offset))
                        y_offset += img.size[1]

                    logger.info(f"PDF ë‹¤ì¤‘ í˜ì´ì§€ í•©ì¹˜ê¸° ì™„ë£Œ. ìµœì¢… í¬ê¸°: {processed_image.size}")
                else:
                    return jsonify({"error": "PDFì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”."}), 400
            else:
                # ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬
                processed_image = Image.open(io.BytesIO(image_bytes))

            # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™” (Gemini API íš¨ìœ¨ì„±ì„ ìœ„í•´)
            max_size = (2048, 2048)
            if processed_image.size[0] > max_size[0] or processed_image.size[1] > max_size[1]:
                processed_image.thumbnail(max_size, Image.Resampling.LANCZOS)
            
            # RGBAë¥¼ RGBë¡œ ë³€í™˜ (PNG íˆ¬ëª…ë„ ì²˜ë¦¬)
            if processed_image.mode == 'RGBA':
                rgb_image = Image.new('RGB', processed_image.size, (255, 255, 255))
                rgb_image.paste(processed_image, mask=processed_image.split()[-1])
                processed_image = rgb_image
            
            logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {processed_image.size}, ëª¨ë“œ: {processed_image.mode}")
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ (ì´ë¯¸ì§€/PDF): {e}")
            return jsonify({"error": f"íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}. poppler-utilsê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."}), 400

        # í”„ë¡œí•„ ì‚¬ì§„ ì¶”ì¶œ ì‹œë„ (ìƒˆë¡œìš´ Gemini ê¸°ë°˜ ë¡œì§ ì‚¬ìš©)
        profile_photo_base64 = detect_and_extract_profile_photo(processed_image)
        
        # ì´ë ¥ì„œ ë°ì´í„° ì¶”ì¶œ
        resume_data = extract_resume_data_from_image(processed_image)
        if not resume_data:
            return jsonify({"error": "ì´ë ¥ì„œì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ê°€ ëª…í™•í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”."}), 500
        
        # í”„ë¡œí•„ ì‚¬ì§„ì´ ì¶”ì¶œë˜ì—ˆë‹¤ë©´ ì¶”ê°€
        if profile_photo_base64:
            # resume_data['profileImageUrl'] = profile_photo_base64 # ê¸°ì¡´ ì½”ë“œëŠ” ì—¬ê¸°ì„œ data ë‚´ë¶€ì— ë„£ì—ˆìŒ
            resume_data['hasProfilePhoto'] = True
            
            # â­ï¸ ìˆ˜ì •: profile_photo_base64ë¥¼ ë”°ë¡œ ê´€ë¦¬í•˜ì—¬ ìµœì¢… ì‘ë‹µì— ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
            # photo_url_to_return ë³€ìˆ˜ì— ì €ì¥
            photo_url_to_return = profile_photo_base64 
        else:
            resume_data['hasProfilePhoto'] = False
            photo_url_to_return = None # í”„ë¡œí•„ ì‚¬ì§„ì´ ì—†ëŠ” ê²½ìš° null ë˜ëŠ” None
            
        # ì´ë ¥ì„œ ì™„ì„±ë„ ë¶„ì„
        completeness_analysis = analyze_resume_completeness(resume_data)
        
        response_data = {
            "success": True,
            "profileImageUrl": photo_url_to_return,
            "data": resume_data,
            "analysis": completeness_analysis,
            "extraction_info": {
                "has_profile_photo": resume_data.get('hasProfilePhoto', False),
                "enhanced": False, # ê°œì„  ë‹¨ê³„ ì œê±°ë¡œ Falseë¡œ ê³ ì •
                "extracted_sections": len([k for k, v in resume_data.items() if v and k != 'hasProfilePhoto']),
                "file_type": file_extension,
                "image_size": f"{processed_image.size[0]}x{processed_image.size[1]}"
            },
            "message": f"âœ… ì´ë ¥ì„œ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ! ì™„ì„±ë„: {completeness_analysis.get('completeness_score', 0)}% (ê°œì„  ë‹¨ê³„ ìƒëµ)"
        }
        
        logger.info(f"ì´ë ¥ì„œ ì¶”ì¶œ ì„±ê³µ - ì™„ì„±ë„: {completeness_analysis.get('completeness_score', 0)}%")
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"API ì˜¤ë¥˜: {e}")
        return jsonify({"error": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

# ğŸ“Œ ë¼ìš°íŠ¸ ì •ì˜ (ë¶„ë¦¬ë¨)

@app.route('/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # Firebase ì—°ê²° ìƒíƒœ ì²´í¬ (Geminië§Œ ì²´í¬)
        firebase_status = "connected" if GEMINI_API_KEY else "disconnected"
        gemini_status = "connected" if GEMINI_API_KEY else "disconnected"
        
        return jsonify({
            "status": "healthy",
            "firebase": firebase_status,
            "gemini": gemini_status,
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/extract-resume', methods=['POST'])
def extract_resume_api():
    """ì´ë¯¸ì§€/PDFì—ì„œ í¬ê´„ì ì¸ ì´ë ¥ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê°•ë ¥í•œ API"""
    logger.info("=== /extract-resume ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ===")
    return _process_resume_extraction()

@app.route('/', methods=['POST'])
def root_extract_resume():
    """ë£¨íŠ¸ ê²½ë¡œì—ì„œë„ ë™ì¼í•œ ê¸°ëŠ¥ ì œê³µ"""
    logger.info("=== / (ë£¨íŠ¸) ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ===")
    return _process_resume_extraction()

# ğŸ“Œ ë””ë²„ê¹…ìš© ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸

@app.route('/test-post', methods=['POST'])
def test_post():
    """POST ìš”ì²­ í…ŒìŠ¤íŠ¸"""
    logger.info("=== /test-post ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ===")
    return jsonify({
        "message": "POST ìš”ì²­ ì„±ê³µ!",
        "received_form_data": dict(request.form),
        "received_files": list(request.files.keys()),
        "method": request.method,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/extract-resume-debug', methods=['POST'])
def extract_resume_debug():
    """ë””ë²„ê·¸ìš© ì—”ë“œí¬ì¸íŠ¸"""
    logger.info("=== /extract-resume-debug ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¨ ===")
    try:
        file = request.files.get('image') or request.files.get('file')
        if not file:
            return jsonify({
                "error": "íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤",
                "files": list(request.files.keys()),
                "form": dict(request.form)
            })
        
        file_size = len(file.read())
        file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
        
        return jsonify({
            "message": "íŒŒì¼ ìˆ˜ì‹  ì„±ê³µ",
            "filename": file.filename,
            "content_type": file.content_type,
            "size": file_size,
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({"error": str(e)})

# NOTE: /enhance-text ì—”ë“œí¬ì¸íŠ¸ì™€ enhance_text í•¨ìˆ˜ëŠ” ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. (ì‹œê°„ ë‹¨ì¶• ëª©ì )

@app.route('/analyze-completeness', methods=['POST'])
def analyze_completeness():
    """ì´ë ¥ì„œ ì™„ì„±ë„ ë¶„ì„ ì „ìš© API"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "ì´ë ¥ì„œ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
            
        analysis = analyze_resume_completeness(data)
        return jsonify({
            "success": True,
            "analysis": analysis
        })
        
    except Exception as e:
        logger.error(f"ì™„ì„±ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return jsonify({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}), 500

# ì—ëŸ¬ í•¸ë“¤ëŸ¬
@app.errorhandler(413)
def too_large(e):
    return jsonify({"error": "íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. 10MB ì´í•˜ì˜ íŒŒì¼ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."}), 413

@app.errorhandler(415)
def unsupported_media_type(e):
    return jsonify({"error": "ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."}), 415

@app.errorhandler(500)
def internal_error(e):
    logger.error(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {e}")
    return jsonify({"error": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500

# ========================================
# ë¼ìš°íŠ¸ ë“±ë¡ í™•ì¸ (ëª¨ë“  ë¼ìš°íŠ¸ ì •ì˜ í›„)
# ========================================
logger.info("="*60)
logger.info("ğŸ”¥ Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ìš°íŠ¸ ë“±ë¡ í™•ì¸")
logger.info("="*60)
for rule in app.url_map.iter_rules():
    methods = ','.join(sorted(rule.methods - {'HEAD', 'OPTIONS'}))
    logger.info(f"âœ… {rule.rule:40s} [{methods:15s}] â†’ {rule.endpoint}")
logger.info("="*60)

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    logger.info(f"ğŸš€ ê°•ë ¥í•œ ì´ë ¥ì„œ AI OCR ì„œë¹„ìŠ¤ ì‹œì‘ - í¬íŠ¸: {port}")
    # Gunicorn ì‚¬ìš©ì„ ê°€ì •í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ ì‹¤í–‰
    # ì‹¤ì œ ë°°í¬ í™˜ê²½ì—ì„œëŠ” Gunicornì´ ì´ íŒŒì¼ì„ í˜¸ì¶œí•¨
    app.run(host='0.0.0.0', port=port, debug=False)